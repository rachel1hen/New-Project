name: Kimi-Audio macOS Test

on:
  workflow_dispatch:

jobs:
  run-kimiaudio:
    runs-on: ubuntu-latest
    container:
      image: nvidia/cuda:12.8.0-devel-ubuntu22.04
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      - name: Install Python3 and pip inside container
        run: |
          apt-get update
          apt-get install -y python3 python3-pip
    
      - name: Set up Python 3.9
        uses: actions/setup-python@v4
        with:
          python-version: 3.9


      - name: Install dependencies and Kimi-Audio package
        run: |
          python3 -m pip install torch torchaudio
          #pip install git+https://github.com/Dao-AILab/flash-attn.git --no-build-isolation
          python3 -m pip install packaging
          python3 -m pip install wheel numpy
          
          python3 -m pip install flash_attn==2.8.3 --no-build-isolation
          
          python3 -m pip install git+https://github.com/MoonshotAI/Kimi-Audio.git
          #git clone https://github.com/MoonshotAI/Kimi-Audio.git
          #cd Kimi-Audio
          #git submodule update --init --recursive
          #pip install -r requirements.txt
          #pip install git+https://github.com/MoonshotAI/Kimi-Audio.git

      - name: Run basic Kimi-Audio example
        run: |
          python -c "
          import torch
           
          from kimi_audio import KimiAudio

          # Load model
          model = KimiAudio.from_pretrained('moonshotai/kimi-audio-v0.1')

          # Dummy audio input (1 second of silence at 16kHz)
          dummy_audio = torch.zeros(1, 16000)

          # Run inference (audio understanding example)
          output = model.audio_to_text(dummy_audio)
          print('Inference output:', output)
          "
